{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7656b5d-a15f-47f6-a945-e6605f06d75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.11.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pdfplumber) (10.0.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber numpy pandas joblib torch transformers nltk scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de5d4c7-8fc1-4075-a86b-69dfc6805244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: seaborn in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.1-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 3.4/8.1 MB 20.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.5/8.1 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 12.8 MB/s eta 0:00:00\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.10.0\n",
      "    Uninstalling matplotlib-3.10.0:\n",
      "      Successfully uninstalled matplotlib-3.10.0\n",
      "Successfully installed matplotlib-3.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mediapipe 0.10.20 requires protobuf<5,>=4.25.3, but you have protobuf 5.29.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de38f83-3ee6-436b-a4b8-e18eb1b98e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Resume Analysis System\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter resume folder path (or press Enter for default D:\\resume_folder):  D:\\resume_folder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the following job description:\n",
      "----------------------------------------\n",
      "Job Description\n",
      "\n",
      "Job Title: Python Developer Intern (Remote)\n",
      "\n",
      "Location: Remote\n",
      "\n",
      "About Us: We are an innovative tech company focused on delivering cutting-edge solutions to a wide range of industries. Our team thrives on creativity, problem-solving, and a shared passion for building impactful products. We are looking for a motivated and eager-to-learn Python Developer Intern to join our dynamic team and contribute to exciting projects.\n",
      "\n",
      "Job Description:\n",
      "\n",
      "As a Python Developer Intern, you will have the opportunity to gain hands-on experience working on real-world projects, learning from senior developers, and contributing to the development of scalable applications. You will work closely with our engineering team to help improve and optimize Python-based solutions, and gain exposure to the latest tools, technologies, and industry best practices.\n",
      "\n",
      "Responsibilities:\n",
      "\n",
      "Assist in developing and maintaining Python applications, scripts, and modules.\n",
      "Write clean, efficient, and well-documented code.\n",
      "Collaborate with team members to troubleshoot, debug, and resolve software defects.\n",
      "Contribute to the design and implementation of new features and functionality.\n",
      "Participate in code reviews and contribute to the improvement of coding standards.\n",
      "Learn and apply best practices for testing, optimization, and performance tuning.\n",
      "Stay up to date with Python developments and emerging technologies in software development.\n",
      "Communicate effectively with team members to ensure smooth project progress.\n",
      "\n",
      "Requirements\n",
      "\n",
      "Requirements:\n",
      "\n",
      "Currently pursuing or recently completed a degree in Computer Science, Software Engineering, or a related field.\n",
      "Basic knowledge of Python programming and its libraries (e.g., Pandas, Flask, Django, etc.).\n",
      "Familiarity with Git or other version control systems.\n",
      "Strong problem-solving skills and a keen eye for detail.\n",
      "Ability to work independently and collaborate in a remote team environment.\n",
      "Eagerness to learn, take on challenges, and improve coding skills.\n",
      "Good communication skills in English (both written and verbal).\n",
      "\n",
      "Preferred Skills (optional But Advantageous):\n",
      "\n",
      "Familiarity with web development frameworks such as Django or Flask.\n",
      "Experience with databases like MySQL, PostgreSQL, or MongoDB.\n",
      "Understanding of REST APIs and web services.\n",
      "Knowledge of front-end technologies (HTML, CSS, JavaScript) is a plus.\n",
      "\n",
      "Requirements\n",
      "\n",
      "Requirements: Currently pursuing or recently completed a degree in Computer Science, Software Engineering, or a related field. Basic knowledge of Python programming and its libraries (e.g., Pandas, Flask, Django, etc.). Familiarity with Git or other version control systems. Strong problem-solving skills and a keen eye for detail. Ability to work independently and collaborate in a remote team environment. Eagerness to learn, take on challenges, and improve coding skills. Good communication skills in English (both written and verbal). Preferred Skills (optional but advantageous): Familiarity with web development frameworks such as Django or Flask. Experience with databases like MySQL, PostgreSQL, or MongoDB. Understanding of REST APIs and web services. Knowledge of front-end technologies (HTML, CSS, JavaScript) is a plus.\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Would you like to use a custom job description? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformers library detected. Will use transformer models for analysis.\n",
      "\n",
      "Initializing analyzer...\n",
      "2025-02-28 15:43:45,269 - ResumeAnalyzer - INFO - NLTK resources initialized successfully\n",
      "\n",
      "Running analysis...\n",
      "2025-02-28 15:43:45,270 - ResumeAnalyzer - INFO - Loading resumes from D:\\resume_folder\n",
      "2025-02-28 15:43:45,271 - ResumeAnalyzer - INFO - Found 3 PDF files\n",
      "Processing 3 PDF files with 3 workers...\n",
      "Processing file 1/3\n",
      "2025-02-28 15:43:45,748 - ResumeAnalyzer - INFO - Successfully loaded 3 resumes\n",
      "2025-02-28 15:43:45,749 - ResumeAnalyzer - INFO - Preprocessing resume texts\n",
      "2025-02-28 15:43:45,756 - ResumeAnalyzer - INFO - Analyzing resumes with TF-IDF\n",
      "2025-02-28 15:43:45,797 - ResumeAnalyzer - INFO - Loading distilbert-base-uncased model\n",
      "2025-02-28 15:43:46,457 - ResumeAnalyzer - INFO - Model loaded on CPU\n",
      "2025-02-28 15:43:46,457 - ResumeAnalyzer - INFO - Analyzing resumes with distilbert-base-uncased\n",
      "Creating embeddings for 3 resumes...\n",
      "Processing embedding 1/3\n",
      "2025-02-28 15:43:47,068 - ResumeAnalyzer - INFO - Results saved to resume_analysis_20250228_154345\\resume_analysis_results.csv\n",
      "2025-02-28 15:43:47,069 - ResumeAnalyzer - INFO - Creating visualizations\n",
      "2025-02-28 15:43:47,198 - ResumeAnalyzer - ERROR - Error creating visualizations: perplexity must be less than n_samples\n",
      "Error creating visualizations: perplexity must be less than n_samples\n",
      "2025-02-28 15:43:47,223 - ResumeAnalyzer - INFO - Model saved to resume_analysis_20250228_154345\\analyzer_model.joblib\n",
      "2025-02-28 15:43:47,238 - ResumeAnalyzer - INFO - Summary report saved to resume_analysis_20250228_154345\\analysis_summary.txt\n",
      "\n",
      "Analysis completed successfully!\n",
      "\n",
      "Top 5 Resume Matches:\n",
      "                        Resume  Combined Score\n",
      "  Md Arif Hussain - Resume.pdf           51.80\n",
      "Md Arif Hussain - RESUME_2.pdf           51.16\n",
      "           Arif_Resume (2).pdf           50.39\n",
      "\n",
      "Complete results saved to: resume_analysis_20250228_154345\n",
      "Check 'resume_analysis_20250228_154345\\analysis_summary.txt' for a detailed report.\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pdfplumber\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import warnings\n",
    "\n",
    "# Suppress Hugging Face symlinks warning\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# Custom progress tracking function to avoid tqdm widget errors\n",
    "def progress_tracker(iterable, desc=None, total=None):\n",
    "    \"\"\"Simple progress tracker that doesn't rely on widgets\"\"\"\n",
    "    if total is None and hasattr(iterable, '__len__'):\n",
    "        total = len(iterable)\n",
    "    \n",
    "    if desc and total:\n",
    "        print(f\"{desc} (total: {total})\")\n",
    "    \n",
    "    for i, item in enumerate(iterable):\n",
    "        if i % 10 == 0 and total:  # Print progress every 10 items\n",
    "            print(f\"Progress: {i}/{total} ({i/total*100:.1f}%)\", end=\"\\r\")\n",
    "        yield item\n",
    "    \n",
    "    if total:\n",
    "        print(f\"Completed: {total}/{total} (100%)\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"resume_analyzer.log\"),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"ResumeAnalyzer\")\n",
    "\n",
    "\n",
    "class ResumeAnalyzer:\n",
    "    def __init__(self, resume_folder, job_description, model_name='distilbert-base-uncased'):\n",
    "        \"\"\"\n",
    "        Initialize the Resume Analyzer with folder path and job description\n",
    "        \n",
    "        Args:\n",
    "            resume_folder (str): Path to folder containing resume PDFs\n",
    "            job_description (str): Job description text to compare against\n",
    "            model_name (str): Transformer model to use for embeddings\n",
    "        \"\"\"\n",
    "        self.resume_folder = resume_folder\n",
    "        self.job_description = job_description\n",
    "        self.model_name = model_name\n",
    "        self.resumes = []\n",
    "        self.resume_names = []\n",
    "        self.stop_words = None\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.vectorizer = None\n",
    "        self.results = None\n",
    "        self.use_transformers = True\n",
    "        \n",
    "        # Create output folder\n",
    "        self.output_folder = f\"resume_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        os.makedirs(self.output_folder, exist_ok=True)\n",
    "        \n",
    "        # Initialize NLTK resources\n",
    "        self._initialize_nltk()\n",
    "    \n",
    "    def _initialize_nltk(self):\n",
    "        \"\"\"Download and initialize NLTK resources\"\"\"\n",
    "        try:\n",
    "            # Use quiet mode to avoid unnecessary output\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            self.stop_words = set(stopwords.words(\"english\"))\n",
    "            logger.info(\"NLTK resources initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing NLTK resources: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"\n",
    "        Extract text from PDF file\n",
    "        \n",
    "        Args:\n",
    "            pdf_path (str): Path to PDF file\n",
    "            \n",
    "        Returns:\n",
    "            str: Extracted text\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \" \"\n",
    "            return text.lower().strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {pdf_path}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess text by tokenizing and removing stopwords\n",
    "        \n",
    "        Args:\n",
    "            text (str): Raw text\n",
    "            \n",
    "        Returns:\n",
    "            str: Preprocessed text\n",
    "        \"\"\"\n",
    "        # Remove special characters and extra whitespace\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Tokenize and remove stopwords\n",
    "        words = word_tokenize(text)\n",
    "        filtered_words = [word for word in words if word.isalnum() and word not in self.stop_words]\n",
    "        \n",
    "        return \" \".join(filtered_words)\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize transformer model for embeddings\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading {self.model_name} model\")\n",
    "            \n",
    "            # Import here to avoid unnecessary loading if transformers aren't used\n",
    "            from transformers import AutoTokenizer, AutoModel\n",
    "            \n",
    "            # Suppress specific transformers warnings\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning, \n",
    "                                   message=\"The cached_file\")\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = AutoModel.from_pretrained(self.model_name)\n",
    "            \n",
    "            # Move model to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                self.model = self.model.to('cuda')\n",
    "                logger.info(\"Model loaded on GPU\")\n",
    "            else:\n",
    "                logger.info(\"Model loaded on CPU\")\n",
    "                \n",
    "            return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading transformer model: {e}\")\n",
    "            logger.info(\"Falling back to TF-IDF vectorization only\")\n",
    "            self.model = None\n",
    "            self.tokenizer = None\n",
    "            self.use_transformers = False\n",
    "            return False\n",
    "    \n",
    "    def _get_transformer_embedding(self, text):\n",
    "        \"\"\"\n",
    "        Get embedding from transformer model\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Embedding vector\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Tokenize and truncate to model's max length\n",
    "            tokens = self.tokenizer(\n",
    "                text, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=512, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move tokens to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                tokens = {key: val.to('cuda') for key, val in tokens.items()}\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**tokens)\n",
    "            \n",
    "            # Use CLS token as sentence embedding\n",
    "            embeddings = outputs.last_hidden_state[:, 0].cpu().numpy()\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating embeddings: {e}\")\n",
    "            return np.zeros((1, 768))  # Return zero vector as fallback\n",
    "    \n",
    "    def load_resumes(self):\n",
    "        \"\"\"Load and process resumes from folder\"\"\"\n",
    "        logger.info(f\"Loading resumes from {self.resume_folder}\")\n",
    "        \n",
    "        if not os.path.isdir(self.resume_folder):\n",
    "            logger.error(f\"Error: '{self.resume_folder}' is not a valid directory\")\n",
    "            raise ValueError(f\"'{self.resume_folder}' is not a valid directory\")\n",
    "        \n",
    "        # Get all PDF files\n",
    "        pdf_files = [f for f in os.listdir(self.resume_folder) if f.lower().endswith(\".pdf\")]\n",
    "        \n",
    "        if not pdf_files:\n",
    "            logger.error(\"No PDF files found in the directory\")\n",
    "            raise ValueError(\"No PDF files found in the directory\")\n",
    "        \n",
    "        logger.info(f\"Found {len(pdf_files)} PDF files\")\n",
    "        \n",
    "        # Process PDFs in parallel with max workers capped to avoid resource issues\n",
    "        max_workers = min(os.cpu_count() or 4, len(pdf_files), 4)  # Limit to max 4 workers\n",
    "        \n",
    "        print(f\"Processing {len(pdf_files)} PDF files with {max_workers} workers...\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            for file in pdf_files:\n",
    "                file_path = os.path.join(self.resume_folder, file)\n",
    "                futures.append(executor.submit(self._extract_text_from_pdf, file_path))\n",
    "            \n",
    "            # Collect results with simple progress tracking\n",
    "            for i, (file, future) in enumerate(zip(pdf_files, futures)):\n",
    "                if i % 5 == 0:\n",
    "                    print(f\"Processing file {i+1}/{len(pdf_files)}\")\n",
    "                    \n",
    "                text = future.result()\n",
    "                if text:\n",
    "                    self.resumes.append(text)\n",
    "                    self.resume_names.append(file)\n",
    "                else:\n",
    "                    logger.warning(f\"No text extracted from {file}\")\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {len(self.resumes)} resumes\")\n",
    "        \n",
    "        # Preprocess texts\n",
    "        logger.info(\"Preprocessing resume texts\")\n",
    "        self.resumes = [self._preprocess_text(resume) for resume in self.resumes]\n",
    "        self.job_description = self._preprocess_text(self.job_description)\n",
    "        \n",
    "        return len(self.resumes)\n",
    "    \n",
    "    def analyze_with_tfidf(self):\n",
    "        \"\"\"Analyze resumes using TF-IDF vectorization\"\"\"\n",
    "        logger.info(\"Analyzing resumes with TF-IDF\")\n",
    "        \n",
    "        # Create and fit vectorizer\n",
    "        self.vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "        all_docs = self.resumes + [self.job_description]\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(all_docs)\n",
    "        \n",
    "        # Get job description vector\n",
    "        job_vector = tfidf_matrix[-1]\n",
    "        resume_vectors = tfidf_matrix[:-1]\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        similarities = cosine_similarity(resume_vectors, job_vector).flatten()\n",
    "        \n",
    "        # Find important keywords\n",
    "        feature_names = np.array(self.vectorizer.get_feature_names_out())\n",
    "        job_keywords = self._extract_important_keywords(job_vector, feature_names)\n",
    "        \n",
    "        # Store results\n",
    "        self.tfidf_results = pd.DataFrame({\n",
    "            \"Resume\": self.resume_names,\n",
    "            \"TF-IDF Similarity\": similarities,\n",
    "            \"ATS Score\": (similarities * 100).round(2)\n",
    "        })\n",
    "        \n",
    "        # Clustering\n",
    "        num_clusters = min(3, len(self.resumes))\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(resume_vectors)\n",
    "        \n",
    "        self.tfidf_results[\"Cluster\"] = cluster_labels\n",
    "        \n",
    "        # Sort by similarity score\n",
    "        self.tfidf_results = self.tfidf_results.sort_values(\n",
    "            by=\"TF-IDF Similarity\", \n",
    "            ascending=False\n",
    "        )\n",
    "        \n",
    "        return self.tfidf_results, job_keywords\n",
    "    \n",
    "    def analyze_with_transformer(self):\n",
    "        \"\"\"Analyze resumes using transformer embeddings\"\"\"\n",
    "        if not self.tokenizer or not self.model:\n",
    "            if not self._initialize_model():\n",
    "                logger.warning(\"Skipping transformer analysis due to model initialization failure\")\n",
    "                return None, []\n",
    "        \n",
    "        logger.info(f\"Analyzing resumes with {self.model_name}\")\n",
    "        \n",
    "        # Get embeddings for all documents\n",
    "        print(f\"Creating embeddings for {len(self.resumes)} resumes...\")\n",
    "        resume_embeddings = []\n",
    "        for i, resume in enumerate(self.resumes):\n",
    "            if i % 5 == 0:\n",
    "                print(f\"Processing embedding {i+1}/{len(self.resumes)}\")\n",
    "            resume_embeddings.append(self._get_transformer_embedding(resume))\n",
    "        \n",
    "        resume_embeddings = np.vstack(resume_embeddings)\n",
    "        job_embedding = self._get_transformer_embedding(self.job_description)\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = cosine_similarity(resume_embeddings, job_embedding).flatten()\n",
    "        \n",
    "        # Store results\n",
    "        self.transformer_results = pd.DataFrame({\n",
    "            \"Resume\": self.resume_names,\n",
    "            \"Transformer Similarity\": similarities,\n",
    "            \"Transformer Score\": (similarities * 100).round(2)\n",
    "        })\n",
    "        \n",
    "        # Sort by similarity score\n",
    "        self.transformer_results = self.transformer_results.sort_values(\n",
    "            by=\"Transformer Similarity\", \n",
    "            ascending=False\n",
    "        )\n",
    "        \n",
    "        return self.transformer_results\n",
    "    \n",
    "    def _extract_important_keywords(self, vector, feature_names, top_n=20):\n",
    "        \"\"\"\n",
    "        Extract important keywords from a TF-IDF vector\n",
    "        \n",
    "        Args:\n",
    "            vector: Sparse vector from TF-IDF\n",
    "            feature_names: Array of feature names\n",
    "            top_n: Number of top keywords to extract\n",
    "            \n",
    "        Returns:\n",
    "            list: List of important keywords\n",
    "        \"\"\"\n",
    "        # Get indices of highest values\n",
    "        indices = vector.toarray().argsort()[0, -top_n:][::-1]\n",
    "        \n",
    "        # Get feature names for these indices\n",
    "        top_keywords = [(feature_names[i], vector[0, i]) for i in indices]\n",
    "        \n",
    "        return top_keywords\n",
    "    \n",
    "    def combine_results(self):\n",
    "        \"\"\"Combine results from different analysis methods\"\"\"\n",
    "        if hasattr(self, 'tfidf_results') and hasattr(self, 'transformer_results') and self.transformer_results is not None:\n",
    "            # Join both result sets\n",
    "            combined = pd.merge(\n",
    "                self.tfidf_results, \n",
    "                self.transformer_results,\n",
    "                on=\"Resume\"\n",
    "            )\n",
    "            \n",
    "            # Calculate combined score (average of both methods)\n",
    "            combined[\"Combined Score\"] = (\n",
    "                (combined[\"ATS Score\"] + combined[\"Transformer Score\"]) / 2\n",
    "            ).round(2)\n",
    "            \n",
    "            # Sort by combined score\n",
    "            combined = combined.sort_values(by=\"Combined Score\", ascending=False)\n",
    "            self.results = combined\n",
    "            \n",
    "        elif hasattr(self, 'tfidf_results'):\n",
    "            self.results = self.tfidf_results\n",
    "            \n",
    "        elif hasattr(self, 'transformer_results') and self.transformer_results is not None:\n",
    "            self.results = self.transformer_results\n",
    "            \n",
    "        else:\n",
    "            logger.error(\"No analysis results available\")\n",
    "            return None\n",
    "        \n",
    "        # Save results to CSV\n",
    "        output_path = os.path.join(self.output_folder, \"resume_analysis_results.csv\")\n",
    "        self.results.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Results saved to {output_path}\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Visualize analysis results\"\"\"\n",
    "        if self.results is None:\n",
    "            logger.error(\"No results available for visualization\")\n",
    "            return\n",
    "        \n",
    "        # Save visualizations to output folder\n",
    "        logger.info(\"Creating visualizations\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Bar chart of top resumes by score\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            top_n = min(10, len(self.results))\n",
    "            \n",
    "            if \"Combined Score\" in self.results.columns:\n",
    "                score_col = \"Combined Score\"\n",
    "            elif \"ATS Score\" in self.results.columns:\n",
    "                score_col = \"ATS Score\"\n",
    "            else:\n",
    "                score_col = \"Transformer Score\"\n",
    "            \n",
    "            # Get top resumes\n",
    "            top_resumes = self.results.head(top_n)\n",
    "            \n",
    "            # Truncate long resume names\n",
    "            top_resumes_plot = top_resumes.copy()\n",
    "            top_resumes_plot[\"Resume\"] = top_resumes_plot[\"Resume\"].apply(\n",
    "                lambda x: x[:25] + \"...\" if len(x) > 25 else x\n",
    "            )\n",
    "            \n",
    "            # Create bar chart\n",
    "            plt.barh(\n",
    "                top_resumes_plot[\"Resume\"],\n",
    "                top_resumes_plot[score_col],\n",
    "                color=sns.color_palette(\"viridis\", top_n)\n",
    "            )\n",
    "            plt.title(f\"Top {top_n} Resumes by {score_col}\")\n",
    "            plt.xlabel(\"Score\")\n",
    "            plt.ylabel(\"Resume\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_folder, \"top_resumes.png\"))\n",
    "            plt.close()\n",
    "            \n",
    "            # 2. Cluster visualization if TF-IDF results available\n",
    "            if hasattr(self, 'tfidf_results') and \"Cluster\" in self.tfidf_results.columns and len(self.resumes) >= 3:\n",
    "                # Get TF-IDF matrix for resumes\n",
    "                tfidf_matrix = self.vectorizer.transform(self.resumes)\n",
    "                \n",
    "                # Use t-SNE for dimensionality reduction\n",
    "                perplexity = min(30, max(5, len(self.resumes) // 2))  # Adjust perplexity based on data size\n",
    "                tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "                reduced_data = tsne.fit_transform(tfidf_matrix.toarray())\n",
    "                \n",
    "                # Create scatter plot\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                \n",
    "                # Get cluster labels\n",
    "                clusters = self.tfidf_results[\"Cluster\"].values\n",
    "                unique_clusters = np.unique(clusters)\n",
    "                \n",
    "                # Plot each cluster\n",
    "                for cluster in unique_clusters:\n",
    "                    indices = np.where(clusters == cluster)[0]\n",
    "                    plt.scatter(\n",
    "                        reduced_data[indices, 0],\n",
    "                        reduced_data[indices, 1],\n",
    "                        label=f\"Cluster {cluster}\",\n",
    "                        alpha=0.7\n",
    "                    )\n",
    "                \n",
    "                # Add resume names as annotations\n",
    "                for i, name in enumerate(self.resume_names):\n",
    "                    short_name = os.path.splitext(name)[0][:15]\n",
    "                    plt.annotate(\n",
    "                        short_name,\n",
    "                        (reduced_data[i, 0], reduced_data[i, 1]),\n",
    "                        fontsize=8\n",
    "                    )\n",
    "                \n",
    "                plt.title(\"Resume Clusters\")\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(self.output_folder, \"resume_clusters.png\"))\n",
    "                plt.close()\n",
    "                \n",
    "            logger.info(\"Visualizations created successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating visualizations: {e}\")\n",
    "            print(f\"Error creating visualizations: {e}\")\n",
    "    \n",
    "    def save_model(self):\n",
    "        \"\"\"Save trained model components for later use\"\"\"\n",
    "        try:\n",
    "            model_path = os.path.join(self.output_folder, \"analyzer_model.joblib\")\n",
    "            \n",
    "            model_data = {\n",
    "                \"vectorizer\": self.vectorizer,\n",
    "                \"job_description\": self.job_description,\n",
    "                \"model_name\": self.model_name\n",
    "            }\n",
    "            \n",
    "            joblib.dump(model_data, model_path)\n",
    "            logger.info(f\"Model saved to {model_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving model: {e}\")\n",
    "    \n",
    "    def extract_missing_keywords(self, resume_idx):\n",
    "        \"\"\"\n",
    "        Find keywords from job description missing in a specific resume\n",
    "        \n",
    "        Args:\n",
    "            resume_idx (int): Index of resume in self.resumes list\n",
    "            \n",
    "        Returns:\n",
    "            list: List of missing keywords\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'vectorizer') or self.vectorizer is None:\n",
    "            logger.error(\"TF-IDF vectorizer not available\")\n",
    "            return []\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Get job description vector\n",
    "        all_docs = self.resumes + [self.job_description]\n",
    "        tfidf_matrix = self.vectorizer.transform(all_docs)\n",
    "        job_vector = tfidf_matrix[-1]\n",
    "        \n",
    "        # Get resume vector\n",
    "        resume_vector = tfidf_matrix[resume_idx]\n",
    "        \n",
    "        # Find important job keywords\n",
    "        job_keywords = self._extract_important_keywords(job_vector, feature_names, top_n=30)\n",
    "        \n",
    "        # Find missing keywords\n",
    "        missing_keywords = []\n",
    "        for keyword, importance in job_keywords:\n",
    "            if resume_vector[0, self.vectorizer.vocabulary_[keyword]] == 0:\n",
    "                missing_keywords.append((keyword, importance))\n",
    "        \n",
    "        return missing_keywords\n",
    "    \n",
    "    def run_full_analysis(self):\n",
    "        \"\"\"Run complete analysis pipeline\"\"\"\n",
    "        # Load resumes\n",
    "        num_resumes = self.load_resumes()\n",
    "        if num_resumes == 0:\n",
    "            logger.error(\"No valid resumes found\")\n",
    "            return None\n",
    "        \n",
    "        # Run TF-IDF analysis\n",
    "        tfidf_results, job_keywords = self.analyze_with_tfidf()\n",
    "        \n",
    "        # Try to run transformer analysis\n",
    "        transformer_results = None\n",
    "        if self.use_transformers:\n",
    "            try:\n",
    "                transformer_results = self.analyze_with_transformer()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Transformer analysis failed: {e}\")\n",
    "                logger.info(\"Continuing with TF-IDF results only\")\n",
    "        \n",
    "        # Combine results\n",
    "        combined_results = self.combine_results()\n",
    "        \n",
    "        # Create visualizations\n",
    "        self.visualize_results()\n",
    "        \n",
    "        # Save model\n",
    "        self.save_model()\n",
    "        \n",
    "        # Create summary report\n",
    "        self._create_summary_report(job_keywords)\n",
    "        \n",
    "        return combined_results\n",
    "    \n",
    "    def _create_summary_report(self, job_keywords):\n",
    "        \"\"\"\n",
    "        Create a summary report of the analysis\n",
    "        \n",
    "        Args:\n",
    "            job_keywords: List of important keywords from job description\n",
    "        \"\"\"\n",
    "        try:\n",
    "            report_path = os.path.join(self.output_folder, \"analysis_summary.txt\")\n",
    "            \n",
    "            with open(report_path, 'w') as f:\n",
    "                f.write(\"RESUME ANALYSIS SUMMARY\\n\")\n",
    "                f.write(\"======================\\n\\n\")\n",
    "                \n",
    "                f.write(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                f.write(f\"Total Resumes Analyzed: {len(self.resumes)}\\n\\n\")\n",
    "                \n",
    "                f.write(\"JOB DESCRIPTION\\n\")\n",
    "                f.write(\"--------------\\n\")\n",
    "                f.write(f\"{self.job_description[:500]}...\\n\\n\")\n",
    "                \n",
    "                f.write(\"TOP JOB KEYWORDS\\n\")\n",
    "                f.write(\"--------------\\n\")\n",
    "                for keyword, importance in job_keywords[:15]:\n",
    "                    f.write(f\"- {keyword} (importance: {importance:.4f})\\n\")\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "                f.write(\"TOP 5 RESUMES\\n\")\n",
    "                f.write(\"-----------\\n\")\n",
    "                for i, (_, row) in enumerate(self.results.head(5).iterrows(), 1):\n",
    "                    f.write(f\"{i}. {row['Resume']}\\n\")\n",
    "                    \n",
    "                    if \"Combined Score\" in row:\n",
    "                        f.write(f\"   Combined Score: {row['Combined Score']:.2f}\\n\")\n",
    "                    if \"ATS Score\" in row:\n",
    "                        f.write(f\"   ATS Score: {row['ATS Score']:.2f}\\n\")\n",
    "                    if \"Transformer Score\" in row:\n",
    "                        f.write(f\"   Transformer Score: {row['Transformer Score']:.2f}\\n\")\n",
    "                    \n",
    "                    # Get missing keywords for this resume\n",
    "                    resume_idx = self.resume_names.index(row['Resume'])\n",
    "                    missing_keywords = self.extract_missing_keywords(resume_idx)\n",
    "                    \n",
    "                    if missing_keywords:\n",
    "                        f.write(\"   Missing Keywords:\\n\")\n",
    "                        for keyword, importance in missing_keywords[:5]:\n",
    "                            f.write(f\"   - {keyword}\\n\")\n",
    "                    \n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "                f.write(\"\\nAnalysis complete. For full details, see the CSV results file.\\n\")\n",
    "            \n",
    "            logger.info(f\"Summary report saved to {report_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating summary report: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Resume Analysis System\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Example usage with compatibility check\n",
    "    resume_folder = input(\"Enter resume folder path (or press Enter for default D:\\\\resume_folder): \")\n",
    "    if not resume_folder:\n",
    "        resume_folder = r\"D:\\resume_folder\"  # Default path\n",
    "    \n",
    "    if not os.path.isdir(resume_folder):\n",
    "        print(f\"Error: '{resume_folder}' is not a valid directory.\")\n",
    "        print(\"Please create this directory and add PDF resumes, or specify a different path.\")\n",
    "        return\n",
    "    \n",
    "    job_description = \"\"\"\n",
    "Job Description\n",
    "\n",
    "Job Title: Python Developer Intern (Remote)\n",
    "\n",
    "Location: Remote\n",
    "\n",
    "About Us: We are an innovative tech company focused on delivering cutting-edge solutions to a wide range of industries. Our team thrives on creativity, problem-solving, and a shared passion for building impactful products. We are looking for a motivated and eager-to-learn Python Developer Intern to join our dynamic team and contribute to exciting projects.\n",
    "\n",
    "Job Description:\n",
    "\n",
    "As a Python Developer Intern, you will have the opportunity to gain hands-on experience working on real-world projects, learning from senior developers, and contributing to the development of scalable applications. You will work closely with our engineering team to help improve and optimize Python-based solutions, and gain exposure to the latest tools, technologies, and industry best practices.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Assist in developing and maintaining Python applications, scripts, and modules.\n",
    "Write clean, efficient, and well-documented code.\n",
    "Collaborate with team members to troubleshoot, debug, and resolve software defects.\n",
    "Contribute to the design and implementation of new features and functionality.\n",
    "Participate in code reviews and contribute to the improvement of coding standards.\n",
    "Learn and apply best practices for testing, optimization, and performance tuning.\n",
    "Stay up to date with Python developments and emerging technologies in software development.\n",
    "Communicate effectively with team members to ensure smooth project progress.\n",
    "\n",
    "Requirements\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Currently pursuing or recently completed a degree in Computer Science, Software Engineering, or a related field.\n",
    "Basic knowledge of Python programming and its libraries (e.g., Pandas, Flask, Django, etc.).\n",
    "Familiarity with Git or other version control systems.\n",
    "Strong problem-solving skills and a keen eye for detail.\n",
    "Ability to work independently and collaborate in a remote team environment.\n",
    "Eagerness to learn, take on challenges, and improve coding skills.\n",
    "Good communication skills in English (both written and verbal).\n",
    "\n",
    "Preferred Skills (optional But Advantageous):\n",
    "\n",
    "Familiarity with web development frameworks such as Django or Flask.\n",
    "Experience with databases like MySQL, PostgreSQL, or MongoDB.\n",
    "Understanding of REST APIs and web services.\n",
    "Knowledge of front-end technologies (HTML, CSS, JavaScript) is a plus.\n",
    "\n",
    "Requirements\n",
    "\n",
    "Requirements: Currently pursuing or recently completed a degree in Computer Science, Software Engineering, or a related field. Basic knowledge of Python programming and its libraries (e.g., Pandas, Flask, Django, etc.). Familiarity with Git or other version control systems. Strong problem-solving skills and a keen eye for detail. Ability to work independently and collaborate in a remote team environment. Eagerness to learn, take on challenges, and improve coding skills. Good communication skills in English (both written and verbal). Preferred Skills (optional but advantageous): Familiarity with web development frameworks such as Django or Flask. Experience with databases like MySQL, PostgreSQL, or MongoDB. Understanding of REST APIs and web services. Knowledge of front-end technologies (HTML, CSS, JavaScript) is a plus.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nUsing the following job description:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(job_description.strip())\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    use_custom_desc = input(\"\\nWould you like to use a custom job description? (y/n): \").lower()\n",
    "    if use_custom_desc == 'y':\n",
    "        print(\"\\nEnter your job description (type 'END' on a new line when finished):\")\n",
    "        lines = []\n",
    "        while True:\n",
    "            line = input()\n",
    "            if line == 'END':\n",
    "                break\n",
    "            lines.append(line)\n",
    "        job_description = '\\n'.join(lines)\n",
    "    \n",
    "    try:\n",
    "        # Check for Hugging Face transformers library\n",
    "        try:\n",
    "            import transformers\n",
    "            use_transformers = True\n",
    "            print(\"\\nTransformers library detected. Will use transformer models for analysis.\")\n",
    "        except ImportError:\n",
    "            use_transformers = False\n",
    "            print(\"\\nTransformers library not found. Will use TF-IDF only for analysis.\")\n",
    "            print(\"To enable transformer models, install with: pip install transformers\")\n",
    "        \n",
    "        # Create analyzer\n",
    "        print(\"\\nInitializing analyzer...\")\n",
    "        analyzer = ResumeAnalyzer(resume_folder, job_description)\n",
    "        analyzer.use_transformers = use_transformers\n",
    "        \n",
    "        # Run analysis\n",
    "        print(\"\\nRunning analysis...\")\n",
    "        results = analyzer.run_full_analysis()\n",
    "        \n",
    "        if results is not None:\n",
    "            print(\"\\nAnalysis completed successfully!\")\n",
    "            print(\"\\nTop 5 Resume Matches:\")\n",
    "            display_cols = [\"Resume\", \"Combined Score\" if \"Combined Score\" in results.columns else \"ATS Score\"]\n",
    "            print(results.head(5)[display_cols].to_string(index=False))\n",
    "            print(f\"\\nComplete results saved to: {analyzer.output_folder}\")\n",
    "            print(f\"Check '{os.path.join(analyzer.output_folder, 'analysis_summary.txt')}' for a detailed report.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Analysis failed: {e}\", exc_info=True)\n",
    "        print(f\"\\nAnalysis failed: {str(e)}\")\n",
    "        print(\"See resume_analyzer.log for more details.\")\n",
    "    \n",
    "    print(\"\\nDone!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a968b3-6742-4af5-abb6-4aa06ceaa850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
